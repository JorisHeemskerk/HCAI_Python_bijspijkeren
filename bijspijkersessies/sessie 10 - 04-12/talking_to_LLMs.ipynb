{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee80849f",
   "metadata": {},
   "source": [
    "# Praten met LLMs\n",
    "\n",
    "Iedereen weet hoe ze moeten praten met LLMs als ChatGPT, Gemini, Claude, etc.: via een website. Dat is voor individueel gebruik natuurlijk perfect, maar wat als je een LLM voor een eigen project in wil zetten? Dan kan dat niet via de browser, dit doen we met een API. Hieronder heb ik een voorbeeldje om te praten met een model van [Nebius Studio](https://tokenfactory.nebius.com/). Dit is een platform waar je gratis 1 euro krijgt om met LLMs te praten. Dit is waarschijnlijk niet genoeg voor je project, maar voor kleine tests werkt dat uitstekend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12a0b8",
   "metadata": {},
   "source": [
    "Eerst de imports dan maar: Nebius werkt op de zelfde manier als Open AI, dit is een soort standaard geworden voor vele LLM API aanbieders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a48887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0547b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7fc8f",
   "metadata": {},
   "source": [
    "Als we werken met een API hebben we een *key* nodig. Dit is een verificatiesleutel die je stuurt naar de API waarmee je laat weten dat jij het bent.\n",
    "\n",
    "Ik heb op de nebius studio website een key aangemaakt en toegevoegd aan mijn windows systeem via de volgende stappen:\n",
    "- Press Start -> type â€œEnvironment Variablesâ€ -> open â€œEdit the system environment variablesâ€\n",
    "- Click Environment Variables\n",
    "- Under User variables (or System variables):\n",
    "  - Click New\n",
    "  - Variable name: HCAI_NEBIUS_API_KEY\n",
    "  - Variable value: your key\n",
    "- Press OK -> restart your notebook\\\n",
    "*(Steps generated with ChatGPT)*\n",
    "\n",
    "<span style=\"color: red; font-weight: bold;\">LET OP: Je wil NOOIT je API key delen. Hiermee kunnen anderen mensen jou geld uitgeven</span>   \n",
    "\n",
    "\n",
    "<span style=\"color: red; font-weight: bold;\">Push de key dus nooit naar GitHub. Ook niet als je deze ergens in een jupyter notebook hebt gebruikt. Dit valt allemaal terug te halen. Reset dan eerst je notebook en zet deze daarna pas in Git.</span>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a33a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEBIUS_API_KEY = os.environ.get(\"HCAI_NEBIUS_API_KEY\") # <- Ik heb mijn API key `HCAI_NEBIUS_API_KEY` genoemd, als die van jou anders heet, pas deze dan aan\n",
    "\n",
    "if not NEBIUS_API_KEY:\n",
    "    raise ValueError(\"NEBIUS_API_KEY is not set in environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509359cc",
   "metadata": {},
   "source": [
    "Nu kunnen we contact maken met de API. Hiervoor maken we een client aan. De client is onze verbinding met de API. We hoeven hier nog niet ons model te specificeren, dat komt straks pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcab353",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=NEBIUS_API_KEY,\n",
    "    base_url=\"https://api.studio.nebius.ai/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e78749",
   "metadata": {},
   "source": [
    "Oke, nu kunnen we een functie gaan schrijven die ene vraag stelt aan het model en het antwoord teruggeeft. Hieronder een voorbeeld:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a40e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_nebius(model: str, prompt: str)-> tuple[object, str, str]:\n",
    "    \"\"\"\n",
    "    Sends a user message to a Nebius Studio LLM and returns the model's reply.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt} \n",
    "        ],\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[ # Hier geven we aan hoe het model de prompt van de user moet ontvangen.\n",
    "            # hier kunnen we een system prompt meegeven. \n",
    "            # Dit kan bijvoorbeeld een instructie zijn voor hoe het model zich moet gedragen tegenover de gebruiker.\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"SYSTEM_PROMPT\"\"\" \n",
    "            },\n",
    "            # Hier geven we de user prompt.\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # De LLM geeft veel informatie terug, maar we zijn alleen geÃ¯nteresseerd in de response tekst. \n",
    "    # We geven eerst alle output, dan alle output als een json format, en daarna alleen de message response.\n",
    "    return response, response.to_json(), response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643dee7f",
   "metadata": {},
   "source": [
    "Oke, laten we een simpele vraag stellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Op dit moment is dit model het goedkoopst\n",
    "model = \"google/gemma-2-2b-it\"\n",
    "response, json_response, text_reply = ask_nebius(model, \"Give me the python code for a hello world example.\")\n",
    "\n",
    "print(\"Model reply:\\n\", text_reply)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96050cf5",
   "metadata": {},
   "source": [
    "Hieronder heb ik de output die ik zelf kreeg geplakt in markdown, zodat de opmaak klopt:\n",
    "\n",
    "---\n",
    "\n",
    "Model reply:\n",
    " ```python\n",
    "print(\"Hello, world!\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **`print()`:** This is a built-in Python function that displays whatever you put inside its parentheses on the screen.\n",
    "* **\"Hello, world!\"** : This is a string, a sequence of characters enclosed within quotation marks.  This is the message you want to display.\n",
    "\n",
    "\n",
    "**To run this code:**\n",
    "\n",
    "1. **Save it:** Create a new file named `hello_world.py` and paste this code into it.\n",
    "2. **Open a terminal:** Navigate to the directory containing the file using the `cd` command.\n",
    "3. **Run the code:** Type `python hello_world.py` and press Enter.\n",
    "\n",
    "\n",
    "You'll see \"Hello, world!\" printed in your terminal. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4dd8ff",
   "metadata": {},
   "source": [
    "---\n",
    "# Prompt tuning\n",
    "Oke, je weet nu hoe je met een LLM moet praten enzo, maar hoe moet je nou de juiste prompt stellen aan ChatGPT? Dit is nog een best lastige vraag, zeker voor complexe vraagstukken. In dit stukje van dit notebook gaan we kijken naar het automatisch trainen van zo'n prompt. \n",
    "\n",
    "Het voorbeeld dat we gebruiken is het omdraaien van de volgorde van letters in een woord. Dus van \"hoi\" naar \"ioh\" bijvoorbeeld.\n",
    "\n",
    "**NOTE:** We gaan hier niet de daadwerkelijke backpropagation bouwen. Dit stukje laat ik aan jullie over en hiervoor kan je prima een bestaande codebase gebruiken of ChatGPT om hulp voor vragen. Dit is complex en dit is iets dat je echt zelf uit zal moeten pluizen.\n",
    "\n",
    "Om prompt tuning uit te voeren maken we gebruik van de `ask_nebius()` functie hierboven. Eerst definiÃ«ren we een kleine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ea07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"cat\", \"tac\"),\n",
    "    (\"hello\", \"olleh\"),\n",
    "    (\"pizza\", \"azzip\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912f898",
   "metadata": {},
   "source": [
    "Dan maken we nu een 'soft prompt', dat is de prompt die we gaan trainen.\n",
    "\n",
    "Hier gebruiken we een hele kleine prompt, met 5 tokens, en 16-dimensionale embeddings.\n",
    "\n",
    "Daarbij maken we een functie die de tokens omzet naar tekst die leesbaar is voor het model. In dit geval gebruiken we hier ook een dummy voorbeeld voor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfafea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_prompt = np.random.randn(5, 16)  \n",
    "\n",
    "def render_soft_prompt(sp):\n",
    "    return \"<SP>\" * len(sp)  # Fake textual standâ€‘in of `<SP>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1322cb1b",
   "metadata": {},
   "source": [
    "Nu kunnen we de training loop bouwen. We updaten de prompt voor 3 epochs, waarbij we een binaire loss definiÃ«ren, die allen kijkt of het klopt of niet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, target):\n",
    "    return sum(a!=b for a,b in zip(pred, target))\n",
    "\n",
    "n_epochs = 3\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for x, y in data:\n",
    "        sp_text = render_soft_prompt(soft_prompt)\n",
    "        _, _, pred = ask_nebius(model, sp_text + \" \" + x)\n",
    "        l = loss(pred, y)\n",
    "        total_loss += l\n",
    "        # Fake gradient update\n",
    "        soft_prompt -= 0.01 * np.sign(np.random.randn(*soft_prompt.shape))\n",
    "    print(f\"Epoch {epoch} loss = {total_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564187e",
   "metadata": {},
   "source": [
    "Waarschijnlijk zien we hier niet dat het model wat geleerd heeft. Dit komt omdat we weinig epochs gebruiken, de gradient niet goed toepassen en deze niet eens echt gebruiken.\n",
    "\n",
    "Toch gaan we even kijken of het heeft gewerkt, met een test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f051d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [\"apple\", \"melon\"]\n",
    "\n",
    "for w in test_words:\n",
    "    sp_text = render_soft_prompt(soft_prompt)\n",
    "    print(w, \"->\", ask_nebius(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", sp_text + \" \" + w)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3246d09",
   "metadata": {},
   "source": [
    "Oke, dit doet het niet, maar dat hadden we ook niet verwacht, dus dat is prima.\n",
    "\n",
    "Wat ik wil laten zien is het idee achter prompt tuning. We geven het model een lijst met random noise als prompt en we gebruiken onze dataset om deze prompt beter en beter te maken, zodat het model de taak uit gaat voeren. \n",
    "\n",
    "Mocht je dit toe willen passen moet je dus op de juiste manier de gradients van je prompt updaten **en** de prompt tokens op de juiste manier meegeven aan het model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec5776",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Finetuning met Nebius\n",
    "\n",
    "Uiteraard kunnen we ook de LLM finetunen in plaats van alleen de prompt. Dit is echter een duurdere operatie, waardoor ik deze niet in dit notebook uit zal voeren. Echter heb ik een fantastische blogpost gevonden die laat zien hoe je voor ongeveer 10 euro LLAMA 3.1 op  de ToolACE dataset kan finetunen, waar 26.500 tekst datapunten in zitten.\n",
    "\n",
    "ðŸ”— https://nebius.com/blog/posts/fine-tuning-llms-with-nebius-ai-studio\n",
    "\n",
    "Hiermee verhogen ze de accuracy van LLAMA 3.1 van 0.842 naar 0.899. Hoewel dat een significante verbetering is, gebruiken ze wel heel erg veel gelabelde data in. De kans dat jullie dit binnen jullie projecten ook hebben is niet zo groot. Waarschijnlijk ga je geen significante verbetering realiseren met een kleine dataset.\n",
    "\n",
    "Daarbij zal je de getrainde LLM ergens moeten hosten. Hiervoor heb je ook een GPU nodig. Dit is altijd duurder dan het gebruik van voorgetrainde LLMs, omdat hier meerdere mensen tegelijk gebruik van kunnen maken, en van je eigen LLM maak je alleen zelf gebruik. Doe dit dus alleen als je een concreet plan hebt hoe en waar je de getrainde LLM gaat hosten en als je weet wat dit gaat kosten. Overleg dit met je opdrachtgever.\n",
    "\n",
    "Tot slot durf ik niet te zeggen of dit data-vriendelijk is en wat Nebius met je data doet. Dit is aan jullie als HCAI studenten om uit te zoeken en te overleggen met de opdrachtgevers.\n",
    "\n",
    "Kortom: deze techniek raad ik dus af."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HCAI_python_TA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
