{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "977d077d",
   "metadata": {},
   "source": [
    "# Extracting specific information from text\n",
    "\n",
    "This notebook shows how the package LangExtract (from Google) can be used to extract information from text data, using LLMs. \n",
    "For this, we use the example of extracting characters, emotions, and relationships from Romeo and Juliet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f5819",
   "metadata": {},
   "source": [
    "First, we have to install the package. Note: we use the sub-package notation `[]` to install the openAI depentencies, which allow us to use Nebius models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdec895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langextract[openai]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f04e2f",
   "metadata": {},
   "source": [
    "Now we import the packages we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4755cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langextract as lx\n",
    "import textwrap\n",
    "from collections import Counter\n",
    "import os\n",
    "from langextract.providers.openai import OpenAILanguageModel\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f5753",
   "metadata": {},
   "source": [
    "This part took me 3 hours to figure out...\n",
    "Here, we declare the model we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = OpenAILanguageModel(\n",
    "    model_id=\"google/gemma-2-9b-it-fast\",\n",
    "    api_key=os.environ.get(\"HCAI_NEBIUS_API_KEY\"), # Replace this with your own API Key if needed.\n",
    "    base_url=\"https://api.tokenfactory.nebius.com/v1/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c08d60",
   "metadata": {},
   "source": [
    "Lets quickly check if we got the right model loaded in. (For me, this line helped A LOT in debugging.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using model: {lm.model_id}\")\n",
    "print(f\"Base URL: {lm.base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f54ebf",
   "metadata": {},
   "source": [
    "Okay, now that we have the right model, we can worry about the data. We can download the entire book, but for the sake of brevity, lets keep it short and take the first part of the book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2220f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the text\n",
    "url = \"https://www.gutenberg.org/files/1513/1513-0.txt\"\n",
    "response = requests.get(url)\n",
    "full_text = response.text\n",
    "\n",
    "# grab the first 5000 chracters\n",
    "partial_text = full_text[:5000]\n",
    "\n",
    "print(\"sample:\\n\", partial_text[200:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240dfe77",
   "metadata": {},
   "source": [
    "Okay, now we have the data, lets define the system prompt.\n",
    "This prompt has been made by copying the [tutorial example](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md), and finetuned to improve consistent output, since we use a much weaker LLM than they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comprehensive prompt and examples for complex literary text\n",
    "prompt = textwrap.dedent(\"\"\"\\\n",
    "    Extract characters, emotions, and relationships from the given text.\n",
    "\n",
    "    Provide meaningful attributes for every entity to add context and depth.\n",
    "\n",
    "    Important: Use exact text from the input for extraction_text. Do not paraphrase.\n",
    "    Extract entities in order of appearance with no overlapping text spans.\n",
    "\n",
    "    Note: In play scripts, speaker names appear in ALL-CAPS followed by a period.\n",
    "                         \n",
    "    Rules:\n",
    "    - extraction_text MUST be a short string from the original text (max 50 characters)\n",
    "    - Use exact text from input, no paraphrasing\n",
    "    - Extract entities in order of appearance\n",
    "    - Each extraction_text must be a simple string value\n",
    "    - In play scripts, speaker names are in ALL-CAPS followed by a period\n",
    "    \n",
    "    Output valid JSON only.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6918de",
   "metadata": {},
   "source": [
    "Now we can add some examples to improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=textwrap.dedent(\"\"\"\\\n",
    "            ROMEO. But soft! What light through yonder window breaks?\n",
    "            It is the east, and Juliet is the sun.\n",
    "            JULIET. O Romeo, Romeo! Wherefore art thou Romeo?\"\"\"),\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"character\",\n",
    "                extraction_text=\"ROMEO\",\n",
    "                attributes={\"emotional_state\": \"wonder\"}\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"emotion\",\n",
    "                extraction_text=\"But soft!\",\n",
    "                attributes={\"feeling\": \"gentle awe\", \"character\": \"Romeo\"}\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"relationship\",\n",
    "                extraction_text=\"Juliet is the sun\",\n",
    "                attributes={\"type\": \"metaphor\", \"character_1\": \"Romeo\", \"character_2\": \"Juliet\"}\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"character\",\n",
    "                extraction_text=\"JULIET\",\n",
    "                attributes={\"emotional_state\": \"yearning\"}\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"emotion\",\n",
    "                extraction_text=\"Wherefore art thou Romeo?\",\n",
    "                attributes={\"feeling\": \"longing question\", \"character\": \"Juliet\"}\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01299524",
   "metadata": {},
   "source": [
    "Okay, now we can actually run it and feed it to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c6296",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lx.extract(\n",
    "    # text_or_documents=\"https://www.gutenberg.org/files/1513/1513-0.txt\", # can also feed it the entire text at once using an URL.\n",
    "    \n",
    "    # Feed the model the document, prompt and examples\n",
    "    text_or_documents=partial_text,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    \n",
    "    # Declare that it should use our defined nebius model\n",
    "    model=lm,\n",
    "\n",
    "    # Hyperparameters:\n",
    "    extraction_passes=1,          # Multiple passes for improved recall\n",
    "    max_workers=1,                # Parallel processing for speed\n",
    "    max_char_buffer=400,          # Smaller contexts for better accuracy\n",
    "    temperature=0.0,              # Lower temperature for more consistent output\n",
    "    fence_output=True,            # no clue what this does\n",
    "    use_schema_constraints=True,  # Enable schema constraints\n",
    "    format_type=\"json\",           # Explicitly specify JSON format\n",
    "    batch_length=1,               # Process one chunk at a time\n",
    "    resolver_params={\n",
    "        \"suppress_parse_errors\": True,  # Continue on parse errors\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted {len(result.extractions)} entities from {len(result.text):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85675542",
   "metadata": {},
   "source": [
    "Some of the outputs might be faulty, since we use a very very weak model, so lets see if we can remove some of the errors with some small post-processing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d054c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_extractions = []\n",
    "for ext in result.extractions:\n",
    "    if isinstance(ext.extraction_text, (str, int, float)) and ext.extraction_text:\n",
    "        valid_extractions.append(ext)\n",
    "    else:\n",
    "        print(f\"Skipping invalid extraction: {ext}\")\n",
    "\n",
    "print(f\"Valid extractions: {len(valid_extractions)}\")\n",
    "result.extractions = valid_extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905410b5",
   "metadata": {},
   "source": [
    "Als je dit meerdere keren runt zal er steeds een andere output uit komen. In mijn voorbeeld kwamen er eers 101 en daarna 71 outputs uit. \n",
    "\n",
    "De LangExtract package kan de resultaten vervolgens voor je visualiseren, dit doen we met de code hieronder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f89a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "if valid_extractions:\n",
    "    lx.io.save_annotated_documents([result], output_name=\"romeo_juliet_extractions.jsonl\", output_dir=\".\")\n",
    "    \n",
    "    # Generate the interactive visualization\n",
    "    html_content = lx.visualize(\"romeo_juliet_extractions.jsonl\")\n",
    "    with open(\"romeo_juliet_visualization.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        if hasattr(html_content, 'data'):\n",
    "            f.write(html_content.data)\n",
    "        else:\n",
    "            f.write(html_content)\n",
    "    \n",
    "    print(\"Interactive visualization saved to romeo_juliet_visualization.html\")\n",
    "else:\n",
    "    print(\"No valid extractions found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad1dcc",
   "metadata": {},
   "source": [
    "Open de HMTL pagina maar eens en kijk hoe goed dit heeft gewerkt!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d8605",
   "metadata": {},
   "source": [
    "Nu kunnen we hieronder nog wat analyses doen op de output data, dit voorbeeldje komt ook gewoon uit de tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze character mentions\n",
    "characters = {}\n",
    "for e in result.extractions:\n",
    "    if e.extraction_class == \"character\":\n",
    "        char_name = e.extraction_text\n",
    "        if char_name not in characters:\n",
    "            characters[char_name] = {\"count\": 0, \"attributes\": set()}\n",
    "        characters[char_name][\"count\"] += 1\n",
    "        if e.attributes:\n",
    "            for attr_key, attr_val in e.attributes.items():\n",
    "                characters[char_name][\"attributes\"].add(f\"{attr_key}: {attr_val}\")\n",
    "\n",
    "# Print character summary\n",
    "print(f\"\\nCHARACTER SUMMARY ({len(characters)} unique characters)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sorted_chars = sorted(characters.items(), key=lambda x: x[1][\"count\"], reverse=True)\n",
    "for char_name, char_data in sorted_chars[:10]:  # Top 10 characters\n",
    "    attrs_preview = list(char_data[\"attributes\"])[:3]\n",
    "    attrs_str = f\" ({', '.join(attrs_preview)})\" if attrs_preview else \"\"\n",
    "    print(f\"{char_name}: {char_data['count']} mentions{attrs_str}\")\n",
    "\n",
    "# Entity type breakdown\n",
    "entity_counts = Counter(e.extraction_class for e in result.extractions)\n",
    "print(f\"\\nENTITY TYPE BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "for entity_type, count in entity_counts.most_common():\n",
    "    percentage = (count / len(result.extractions)) * 100\n",
    "    print(f\"{entity_type}: {count} ({percentage:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HCAI_python_TA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
